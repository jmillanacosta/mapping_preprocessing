# Workflow for downloading and saving Wikidata secondary2primary mappings
name: wikidata

on:
  workflow_dispatch:
  ##TODO: add schedule for deployment (suggestion: once a month?)
  
jobs:
  wikidata:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      # step 1: checkout the repository
    steps:
      - name: Download GitHub repo for the queries
        uses: actions/checkout@v3

      # step 2: run the SPARQL queries from the Wikidata query subfolder
      - name: Run the Queries
        run: |
          ##Make directory if not existing already
          mkdir datasources/Wikidata/results
          ## Download outdated IDs for chemicals
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/Wikidata/queries/chemicalRedirects.rq -G https://query.wikidata.org/sparql -o datasources/Wikidata/results/metabolites_secID2priID.tsv
          ## Download all primary IDs for chemicals
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/Wikidata/queries/chemicalAllPrimary.rq -G https://query.wikidata.org/sparql -o datasources/Wikidata/results/metabolites_priIDs.tsv
          ## Download alias/synonyms/names for chemicals
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/Wikidata/queries/chemicalPrimarySynonyms.rq -G https://query.wikidata.org/sparql -o datasources/Wikidata/results/metabolites_name2synonym.tsv
          
          ## Download outdated IDs for genes and proteins
          #curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/Wikidata/queries/geneproteinHumanRedirects.rq -G https://query.wikidata.org/sparql -o datasources/Wikidata/results/geneProtein_secID2priID.tsv
          ## Download all primary IDs for genes and proteins
          #curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/Wikidata/queries/geneproteinHumanAllPrimary.rq -G https://query.wikidata.org/sparql -o datasources/Wikidata/results/geneProtein_priIDs.tsv
          ## Download alias/synonyms/names for genes and proteins
          #curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/Wikidata/queries/geneproteinHumanPrimarySynonyms.rq -G https://query.wikidata.org/sparql -o datasources/Wikidata/results/geneProtein_name2synonym.tsv

          ##Check new data, fail job if query timeout has occured
          cd datasources/Wikidata/results
          for File in *.tsv ##Only for tsv files
          do
            if grep -q TimeoutException "$File"; then
              echo "Query Timeout occurred, Wikidata data will not be updated"
              exit 1
            fi
          done
          
          ##Remove previous output files (if existing)
          ##find  . -name 'Wikidata*' -exec rm {} \;
          ## Set prefix to Wikidata for renaming new data files
          prefix=$(basename "Wikidata") 
          for f in *.tsv ##Only for tsv files
          do
            ##Find all new data files | Remove the IRIs (prefix) | remove the IRIs (suffix) | remove language annotation | save the file with new name
            cat "$f" | sed 's/<http:\/\/www.wikidata.org\/entity\///g' | sed 's/[>]//g' | sed 's/@en//g' > "${prefix}_$f"
            rm "$f"
          done
          ##Change back to main directory
          cd ../..
          ##Move and overwrite all files from results folder to data folder, to update previous data
          mv -f Wikidata/results/* Wikidata/data/
      # step 3: save the data from the queries
      - name: Commit and Push Changes
        run: |
          git pull
          ls
          git add .
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git commit -m "Updating Wd data"
          git push
