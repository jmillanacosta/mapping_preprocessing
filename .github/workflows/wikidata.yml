# Workflow for downloading and saving Wikidata secondary2primary mappings
name: wikidata

on:
  workflow_dispatch:
  ##TODO: add schedule for deployment (suggestion: once a month?)
  
jobs:
  wikidata:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      # step 1: checkout the repository
    steps:
      - name: Download GitHub repo for the queries
        uses: actions/checkout@v3

      # step 2: run the SPARQL queries from the Wikidata query subfolder
      - name: Run the Queries
        run: |
          ##Make directory if not existing already
          mkdir datasources/wikidata/results
          ##Define variable to be used in storing and updating output data (to avoid hardcoding for each change) (tba)
          
          ## Download outdated IDs for chemicals
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/chemicalRedirects.rq -G https://query.wikidata.org/sparql -o datasources/wikidata/results/metabolites_secID2priID.tsv
          ## Download all primary IDs for chemicals
          #curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/chemicalAllPrimary.rq -G https://query.wikidata.org/sparql -o datasources/wikidata/results/metabolites_priIDs.tsv
          ## Download alias/synonyms/names for chemicals
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/chemicalPrimarySynonyms.rq -G https://query.wikidata.org/sparql -o datasources/wikidata/results/metabolites_name2synonym.tsv
          
          ## Download outdated IDs for genes (split from proteins to avoid timeouts)
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/geneHumanRedirects.rq -G https://query.wikidata.org/sparql -o datasources/wikidata/results/gene_secID2priID.tsv
          ## Download outdated IDs for proteins (split from genes to avoid timeouts)
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/proteinHumanRedirects.rq -G https://query.wikidata.org/sparql -o datasources/wikidata/results/protein_secID2priID.tsv
          ## Download all primary IDs for genes and proteins
          #curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/geneproteinHumanAllPrimary.rq -G https://query.wikidata.org/sparql -o datasources/wikidata/results/geneProtein_priIDs.tsv
          ## Download alias/synonyms/names for genes and proteins
          #curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/geneproteinHumanPrimarySynonyms.rq -G https://query.wikidata.org/sparql -o datasources/wikidata/results/geneProtein_name2synonym.tsv

          ##Concatenate gene and protein outdated ID data
          head -n 1 datasources/wikidata/results/gene_secID2priID.tsv > datasources/wikidata/results/geneProtein_secID2priID.tsv ##Add the header of one file as start
          tail -n +2 -q datasources/wikidata/results/gene_secID2priID.tsv >> datasources/wikidata/results/geneProtein_secID2priID.tsv ##Add gene sec. IDs to the file (not overwrite)
          tail -n +2 -q datasources/wikidata/results/protein_secID2priID.tsv >> datasources/wikidata/results/geneProtein_secID2priID.tsv ##Add protein sec. IDs to the file (not overwrite)
          
          ##Check new data, fail job if query timeout has occured
          cd datasources/wikidata/results
          for File in *.tsv ##Only for tsv files
          do
            if grep -q TimeoutException "$File"; then
              echo "Query Timeout occurred for file: " "$File" 
              echo "Wikidata data will not be updated"
              head -n 20 "$File" 
              exit 1
            else
              echo "No Query Timeout detected for file: " "$File" 
            fi
          done
          
          ##Remove previous output files (if existing)
          ##find  . -name 'wikidata*' -exec rm {} \;
          ## Set prefix to Wikidata for renaming new data files
          prefix=$(basename "Wikidata") 
          for f in *.tsv ##Only for tsv files
          do
            ##Find all new data files | Remove the IRIs (prefix) | remove the IRIs (suffix) | remove language annotation | save the file with new name
            cat "$f" | sed 's/<http:\/\/www.wikidata.org\/entity\///g' | sed 's/[>]//g' | sed 's/@en//g' > "${prefix}_$f"
            rm "$f"
          done
          ##Change back to main directory
          cd ../..
          ##Move and overwrite all files from results folder to data folder, to update previous data
          mv -f wikidata/results/* wikidata/data/
      # step 3: save the data from the queries
      - name: Commit and Push Changes
        run: |
          git pull
          ls
          git add .
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git commit -m "Updating Wd data"
          git push
